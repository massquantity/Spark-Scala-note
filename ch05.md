```scala
import org.apache.spark.sql.{DataFrame, SparkSession}
import scala.util.Random

val data = spark.read.
  option("inferSchema", true).
  option("header", false).
  csv("hdfs:///user/massquantity/kddcup_small.data").
   toDF(
    "duration", "protocol_type", "service", "flag",
    "src_bytes", "dst_bytes", "land", "wrong_fragment", "urgent",
    "hot", "num_failed_logins", "logged_in", "num_compromised",
    "root_shell", "su_attempted", "num_root", "num_file_creations",
    "num_shells", "num_access_files", "num_outbound_cmds",
    "is_host_login", "is_guest_login", "count", "srv_count",
    "serror_rate", "srv_serror_rate", "rerror_rate", "srv_rerror_rate",
    "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
    "dst_host_count", "dst_host_srv_count",
    "dst_host_same_srv_rate", "dst_host_diff_srv_rate",
    "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate",
    "dst_host_serror_rate", "dst_host_srv_serror_rate",
    "dst_host_rerror_rate", "dst_host_srv_rerror_rate",
    "label")

data.cache()
data.select("label").groupBy("label").count().orderBy($"count".desc).show(25)

import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.clustering.{KMeans, KMeansModel}
import org.apache.spark.ml.feature.VectorAssembler

val numericOnly = data.drop("protocol_type", "service", "flag").cache()
val assembler = new VectorAssembler().
  setInputCols(numericOnly.columns.filter(_ != "label")).
  setOutputCol("featureVector")
val kmeans = new KMeans().
  setPredictionCol("cluster").
  setFeaturesCol("featureVector")

val pipeline = new Pipeline().setStages(Array(assembler, kmeans))
val pipelineModel = pipeline.fit(numericOnly)
val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]
kmeansModel.clusterCenters.foreach(println)

val withCluster = pipelineModel.transform(numericOnly)
withCluster.select("cluster", "label").
  groupBy("cluster", "label").count().
  orderBy($"cluster", $"count".desc).
  show(25)
```

```scala
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.ml.{PipelineModel, Pipeline}

def clusterScore0(data: DataFrame, k: Int): Double = {
  val assembler = new VectorAssembler().
    setInputCols(data.columns.filter(_ != "label")).
    setOutputCol("featureVector")
  val scaler = new StandardScaler().
  	setInputCol("featureVector").
  	setOutputCol("scaledFeatureVector").
  	setWithStd(true).
  	setWithMean(false)
  val kmeans = new KMeans().
    setSeed(Random.nextLong()).
    setK(k).
    setMaxIter(40).
    setTol(1e-5).
    setPredictionCol("cluster").
    setFeaturesCol("scaledFeatureVector")
  val pipeline = new Pipeline().setStages(Array(assembler, kmeans))
  val pipelineModel = pipeline.fit(data)
  val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]
  kmeansModel.computeCost(assembler.transform(data)) / data.count()
}

(20 to 100 by 20).map(k => (k, clusterScore0(numericOnly, k))).foreach(println)
```

In fact, one-hot-encoding string-valued features like protocol_type is actually a two-step process. First, the string values are converted to integer indices like 0, 1, 2, and so on using **StringIndexer**. Then these integer indices are encoded into a vector with **OneHotEncoder**.

```scala
// groupBy & mapGroups example
val clusterLabel =
pipelineModel.transform(data).select("cluster", "label").as[(Int, String)]
// clusterLabels is an Iterator[(Int, String)]
clusterLabel.groupByKey{case (cluster, _) => cluster }.mapGroups {case (c, clusterLabels) => (c, clusterLabels.toArray)}.collect()

Array[(Int, Array[(Int, String)])] = Array(
  (1,Array((1,portsweep.))), 
  (0,Array((0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), 	  (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal.), (0,normal...
```

```scala
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, StandardScaler}

def oneHotPipeline(inputCol: String): (Pipeline, String) = {
  val indexer = new StringIndexer().
    setInputCol(inputCol).
    setOutputCol(inputCol + "_indexed")
  val encoder = new OneHotEncoder().
    setInputCol(inputCol + "_indexed").
    setOutputCol(inputCol + "_vec")
  val pipeline = new Pipeline().setStages(Array(indexer, encoder))
  (pipeline, inputCol + "_vec")
}

def clusteringScore(data: DataFrame, k: Int): Double = {
  val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline("protocol_type")
  val (serviceEncoder, serviceVecCol) = oneHotPipeline("service")
  val (flagEncoder, flagVecCol) = oneHotPipeline("flag")

  val assembleCols = Set(data.columns:_*) -- 
    Seq("label", "protocol_type", "service", "flag") ++ 
    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)
  val assembler = new VectorAssembler().
    setInputCols(assembleCols.toArray).
    setOutputCol("featureVector")
  val scaler = new StandardScaler().
    setInputCol("featureVector").
    setOutputCol("scaledFeatureVector").
    setWithStd(true).
    setWithMean(false)
  val kmeans = new KMeans().
    setSeed(Random.nextLong()).
    setK(k).
    setPredictionCol("cluster").
    setFeaturesCol("scaledFeatureVector").
    setMaxIter(40).
    setTol(1.0e-5)

  val pipeline = new Pipeline().setStages(
    Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))
  val pipelineModel = pipeline.fit(data)
  val kmeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]
  kmeansModel.computeCost(pipelinrModel.transform(data)) / data.count()
}
```

```scala
def entropy(counts: Iterable[Int]): Double = {
  val values = counts.filter(_ > 0)
  val n = values.map(_.toDouble).sum
  values.map { v =>
    val p = v / n
    -p * math.log(p)
  }.sum
}

def fitPipeline4(data: DataFrame, k: Int): PipelineModel = {
  val (protoTypeEncoder, protoTypeVecCol) = oneHotPipeline("protocol_type")
  val (serviceEncoder, serviceVecCol) = oneHotPipeline("service")
  val (flagEncoder, flagVecCol) = oneHotPipeline("flag")

  // Original columns, without label / string columns, but with new vector encoded cols
  val assembleCols = Set(data.columns: _*) --
    Seq("label", "protocol_type", "service", "flag") ++
    Seq(protoTypeVecCol, serviceVecCol, flagVecCol)
  val assembler = new VectorAssembler().
    setInputCols(assembleCols.toArray).
    setOutputCol("featureVector")

  val scaler = new StandardScaler()
    .setInputCol("featureVector")
    .setOutputCol("scaledFeatureVector")
    .setWithStd(true)
    .setWithMean(false)

  val kmeans = new KMeans().
    setSeed(Random.nextLong()).
    setK(k).
    setPredictionCol("cluster").
    setFeaturesCol("scaledFeatureVector").
    setMaxIter(40).
    setTol(1.0e-5)

  val pipeline = new Pipeline().setStages(
    Array(protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans))
  pipeline.fit(data)
}

def clusteringScore4(data: DataFrame, k: Int): Double = {
  val pipelineModel = fitPipeline4(data, k)

  val clusterLabel = pipelineModel.transform(data).
    select("cluster", "label").as[(Int, String)]
  val weightedClusterEntropy = clusterLabel.
    groupByKey { case (cluster, _) => cluster }.
    mapGroups { case (_, clusterLabels) => 
      val labels = clusterLabels.map { case (_, label) => label }.toSeq
      val labelCounts = labels.groupBy(identity).values.map(_.size)
      labels.size * entropy(labelCounts)
  }.collect()

  weightedClusterEntropy.sum / data.count()
}

(60 to 270 by 30).map(k => (k, clusteringScore4(data, k))).foreach(println)
```

```scala
val pipelineModel = fitPipeline4(data, 180)
val countByClusterLabel = pipelineModel.transform(data).
  select("cluster", "label").
  groupBy("cluster", "label").count().
  orderBy("cluster", "label")
countByClusterLabel.show()
```

![](https://raw.githubusercontent.com/massquantity/Spark-advanced/master/pic/5.png)

Now we can make an actual anomaly detector. Anomaly detection amounts to measuring a new data pointâ€™s distance to its nearest centroid. If this distance exceeds some threshold, it is anomalous. This threshold might be chosen to be the distance of, say, the 100th-farthest data point from among known data :

```scala
import org.apache.spark.ml.linalg.{Vector, Vectors}

val kMeansModel = pipelineModel.stages.last.asInstanceOf[KMeansModel]
val centroids = kMeansModel.clusterCenters

val clustered = pipelineModel.transform(data)
val threshold = clustered.select("cluster", "scaledFeatureVector").as[(Int, Vector)].
  map { case (cluster, vec) => Vectors.sqdist(centroids(cluster), vec) }.
  orderBy($"value".desc).take(100).last

// check anomalies
val originalCols = data.columns
val anomalies = clustered.filter {row => 
  val cluster = row.getAs[Int]("cluster")
  val vec = row.getAs[Vector]("scaledFeatureVector")
  Vectors.sqdist(centroids(cluster), vec) >= threshold
}.select(originalCols.head, originalCols.tail:_*)

println(anomalies.first())
```









































