```scala
val dataWithoutHeader = spark.read.option("inferSchema", true).option("header", false).csv("hdfs:///user/massquantity/covtype.data")

val colNames =  Seq(
  "Elevation", "Aspect", "Slope",
  "Horizontal_Distance_To_Hydrology", "Vertical_Distance_To_Hydrology",
  "Horizontal_Distance_To_Roadways",
  "Hillshade_9am", "Hillshade_Noon", "Hillshade_3pm",
  "Horizontal_Distance_To_Fire_Points"
) ++ (
  (0 until 4).map(i => s"Wilderness_Area_$i")
) ++ (
  (0 until 40).map(i => s"Soil_Type_$i")
) ++ Seq("Cover_Type")

val data = dataWithoutHeader.toDF(colNames:_*).
  withColumn("Cover_Type", $"Cover_Type".cast("double"))

val Array(trainData, testData) = data.randomSplit(Array(0.9, 0.1))
trainData.cache()
testData.cache()

import org.apache.spark.ml.feature.VectorAssembler
val inputCols = trainData.columns.filter(_ != "Cover_Type")
val assembler = new VectorAssembler().
  setInputCols(inputCols).
  setOutputCol("featureVector")
val assembledTrainData = assembler.transform(trainData)
assembledTrainData.select("featureVector").show(5, truncate = false)
```

The output doesn’t look exactly like a sequence of numbers, but that’s because this shows a raw representation of the vector, represented as a **SparseVector** instance to save storage. 

```scala
import org.apache.spark.ml.classification.DecisionTreeClassifier
import scala.util.Random

val classifier = new DecisionTreeClassifier().
  setSeed(Random.nextLong()).
  setLabelCol("Cover_Type").
  setFeaturesCol("featureVector").
  setPredictionCol("prediction")
val model = classifier.fit(assembledTrainData)
println(model.toDebugString)

model.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)
```

![](https://raw.githubusercontent.com/massquantity/Spark-advanced/master/pic/3.png)



```scala
val predictions = model.transform(assembledTrainData)
predictions.select("Cover_Type", "prediction", "probability").
show(truncate = false)

import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val evaluator = new MulticlassClassificationEvaluator().
  setLabelCol("Cover_Type").
  setPredictionCol("prediction")
evaluator.setMetricName("accuracy").evaluate(predictions)
evaluator.setMetricName("f1").evaluate(predictions)
```

Fortunately, Spark provides support code to compute the **confusion matrix**. Unfortunately, that implementation exists as part of the older MLlib APIs that operate on RDDs. However, that’s no big deal, because **data frames and data sets can freely be turned into RDDs** and used with these older APIs. Here, MulticlassMetrics is appropriate for a data frame containing predictions.

```scala
import org.apache.spark.mllib.evaluation.MulticlassMetrics
val predictionRDD = predictions.
	select("prediction", "Cover_Type").
	as[(Double,Double)].  // convert to Dataset
	rdd  									// convert to RDD

val multiclassMetrics = new MulticlassMetrics(predictionRDD)
multiclassMetrics.confusionMatrix

val confusionMatrix = predictions.  // DataFrame way to compute confusion matrix
  groupBy("Cover_Type").
  pivot("prediction", (1 to 7)).
  count().
  na.fill(0.0).
  orderBy("Cover_Type")
confusionMatrix.show()
```



We could construct such a random “classifier” by picking a class at random in proportion to its prevalence in the training set. For example, if 30% of the training set were cover type 1, then the random classifier would guess “1” 33% of the time. Each classification would be correct in proportion to its prevalence in the test set. If 40% of the test set were cover type 1, then guessing “1” would be correct 40% of the time. Cover type 1 would then be guessed correctly 30% x 40% = 12% of the time and contribute 12% to overall accuracy. Therefore, we can evaluate the accuracy by summing these products of probabilities:

```scala
import org.apache.spark.sql.DataFrame
def classProbabilities(data: DataFrame): Array[Double] = {
  val total = data.count()
  data.groupBy("Cover_Type").count().
  orderBy("Cover_Type").
  select("count").as[Double].
  map(_ / total).
  collect()
}

val trainPriorProbabilities = classProbabilities(trainData)
val testPriorProbabilities = classProbabilities(testData)
trainPriorProbabilities.zip(testPriorProbabilities).map {
  case (trainProb, cvProb) => trainProb * cvProb
}.sum   // 0.37
```

<br>

### Categorical Features Revisited

**One-Hot encoding**  forces the decision tree algorithm to consider the values of the underlying categorical features individually. Because features like soil type are broken down into many features, and because decision trees treat features individually, it is harder to relate information about related soil types.

For example, **nine different soil types are actually part of the Leighcan family**, and they may be related in ways that the decision tree can exploit. If soil type were encoded as a single categorical feature with 40 soil values, then the tree could express rules like “if the soil type is one of the nine Leighton family types” directly. However, when encoded as 40 features, the tree would have to learn a sequence of nine decisions on soil type to do the same, this expressiveness may lead to better decisions and more efficient trees.

However, having 40 numeric features represent one 40-valued categorical feature
increases memory usage and slows things down. What about undoing the one-hot encoding? This would replace, for example, the four columns encoding wilderness type with one column that encodes the wilderness type as a number between 0 and 3, like “Cover_Type”.

```scala
import org.apache.spark.ml.{PipelineModel, Pipeline}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.functions._

def unencodeOneHot(data: DataFrame): DataFrame = {
  val wildernessCols = (0 until 4).map(i => s"Wilderness_Area_$i").toArray
  val wildernessAssembler = new VectorAssembler().
    setInputCols(wildernessCols).
    setOutputCol("wilderness")
  
  val unhotUDF = udf((vec: Vector) => vec.toArray.indexOf(1.0).toDouble)
  val withWilderness = wildernessAssembler.transform(data).
    drop(wildernessCols:_*).
    withColumn("wilderness", unhotUDF($"wilderness"))

  val soilCols = (0 until 40).map(i => s"Soil_Type_$i").toArray
  val soilAssembler = new VectorAssembler().
    setInputCols(soilCols).
    setOutputCol("soil")
  
  soilAssembler.transform(withWilderness).
    drop(soilCols:_*).
    withColumn("soil", unhotUDF($"soil"))
}

import org.apache.spark.ml.feature.VectorIndexer
val unencTrainData = unencodeOneHot(trainData)
val unencTestDatae = unencodeOneHot(testData)

val inputCols = unencTrainData.columns.filter(_ != "Cover_Type")
val assembler = new VectorAssembler().
  setInputCols(inputCols).
  setOutputCol("featureVector")

val indexer = new VectorIndexer().
  setMaxCategories(40).
  setInputCol("featureVector").
  setOutputCol("indexedVector")

val classifier = new DecisionTreeClassifier().
  setSeed(Random.nextLong()).
  setLabelCol("Cover_Type").
  setFeaturesCol("indexedVector").
  setPredictionCol("prediction")

val pipeline = new Pipeline().setStages(Array(assembler, indexer, classifier))
```

```scala
import org.apache.spark.ml.tuning.ParamGridBuilder
val paramGrid = new ParamGridBuilder().
	addGrid(classifier.impurity, Seq("gini", "entropy")).
	addGrid(classifier.maxDepth, Seq(1, 20)).
	addGrid(classifier.maxBins, Seq(40, 300)).
	addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).
	build()

val multiclassEval = new MulticlassClassificationEvaluator().
	setLabelCol("Cover_Type").
	setPredictionCol("prediction").
	setMetricName("accuracy")

import org.apache.spark.ml.tuning.TrainValidationSplit
val validator = new TrainValidationSplit().
	setSeed(Random.nextLong()).
	setEstimator(pipeline).
	setEvaluator(multiclassEval).
	setEstimatorParamMaps(paramGrid).
	setTrainRatio(0.9)

val validatorModel = validator.fit(trainData)  // validatorModel

val bestModel = validatorModel.bestModel
println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)

val testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))
println(testAccuracy)
```

```scala
val paramsAndMetrics = validatorModel.validationMetrics.
  zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)
paramsAndMetrics.foreach { case (metric, params) => 
  println(metric)
  println(params)
  println()
}
```

![](https://raw.githubusercontent.com/massquantity/Spark-advanced/master/pic/4.png)



```scala
val classifier = new RandomForestClassifier().
	setSeed(Random.nextLong()).
	setLabelCol("Cover_Type").
	setFeaturesCol("indexedVector").
	setPredictionCol("prediction").
	setImpurity("entropy").
	setMaxDepth(20).
	setMaxBins(300)


val bestModel = validatorModel.bestModel
val forestModel = bestModel.asInstanceOf[PipelineModel].
	stages.last.asInstanceOf[RandomForestClassificationModel]

println(forestModel.extractParamMap)
println(forestModel.getNumTrees)
forestModel.featureImportances.toArray.zip(inputCols).
	sorted.reverse.foreach(println)

val testAccuracy = multiclassEval.evaluate(bestModel.transform(unencTestData))
println(testAccuracy)
bestModel.transform(unencTestData.drop("Cover_Type")).select("prediction").show()
```























