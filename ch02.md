```shell
$ hadoop fs -mkdir linkage
$ hadoop fs -put block_*.csv linkage

$ spark-shell --master yarn --deploy-mode client
$ spark-shell --master local[*]
```

```scala
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
import org.apache.spark.sql.functions._ // for lit(), first(), etc.

def isHeader(line: String): Boolean = line.contains("id_1")
head.filterNot(isHeader).length
head.filter(x => !isHeader(x)).length
head.filter(!isHeader(_)).length

val spark = SparkSession.builder.appName("ch02").getOrCreate
val preview = spark.read.csv("hdfs:///user/massquantity/linkage")
val parsed = spark.read
  .option("header", "true")
  .option("nullValue", "?")
  .option("inferSchema", "true")
  .csv("hdfs:///user/massquantity/linkage")

parsed.count()
parsed.cache()

```

The call to cache indicates that the contents of the DataFrame should be stored in memory the next time it’s computed. In this example, the call to count computes the contents initially.  



Deciding when to cache data can be an art. The decision typically involves trade-offs between space and speed, with the specter of garbage-collecting looming overhead to occasionally confound things further. In general, data should be cached when it is likely to be referenced by multiple actions, is relatively small compared to the amount of memory/disk available on the cluster, and is expensive to regenerate.

```scala
parsed.rdd.   // rdd version
  map(_.getAs[Boolean]("is_match")).
  countByValue()
// Map(true -> 20931, false -> 5728201)

parsed.
  groupBy("is_match").
  count().
  orderBy("$count".desc).
  show()
```

Under the covers, the Spark engine determines the most efficient way to perform the aggregation and return the results, without us having to worry about the details of which RDD APIs to use. The result is a cleaner, faster, and more expressive way to do data analysis in Spark.

Note that there are two ways we can reference the names of the columns in the DataFrame: either as literal strings, like in groupBy("is_match"), or as Column objects by using the special "_\<col\>_" syntax that we used on the count column. Either approach is valid in most cases, but we needed to use the \$ syntax to call the desc method on the count column. If we had omitted the \$  in front of the string, Scala would have thrown an error because the String class does not have a method named desc.



```scala
// to use spark sql, first create Temp View
parsed.createOrReplaceTempView("linkage")
spark.sql("""
  SELECT is_match, COUNT(*) cnt
  FROM linkage
  GROUP BY is_match
  ORDER BY cnt DESC
""").show()
```

Should you use Spark SQL or the DataFrame API to do your analysis in Spark? There are pros and cons to each: SQL has the benefit of being broadly familiar and expressive for simple queries. It is also the best way to quickly read and filter data stored in commonly used columnar file formats like ORC and Parquet. The downside of SQL is that it can be difficult to express complex, multistage analyses in a dynamic, readable, and testable way—all areas where the DataFrame API shines. 

![](https://raw.githubusercontent.com/massquantity/Spark-advanced/master/pic/1.png?token=AG3LPEGSSLND2DEEST3SFYS5EW6RA)



```scala
val summary = parsed.describe()
summary.select("summary", "cmp_fname_c1", "cmp_fname_c2").show()
val matches = parsed.where("is_match = true")
val misses = parsed.filter($"is_match" === false)
```



A flatMap is one of the most useful transforms in Spark: it takes a function argument that processes each input record and returns __a sequence of__  zero or more output records.  (result must be a sequence)

```scala
// Pivot.scala
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions.first

def pivotSummary(desc: DataFrame): DataFrame = {
  val schema = desc.schema
  import desc.sparkSession.implicits._
  
  val lf = desc.flatMap (row => {
    val metric = row.getString(0)
    (1 until row.size).map(i => {
      (metric, schema(i).name, row.getString(i).toDouble) 
      //  toDouble is an example of implicit types
    }) 	
  }).toDF("metric", "field", "value")
    
  lf.groupBy("field").
    pivot("metric", Seq("count", "mean", "stddev", "min", "max")).
    agg(first("value"))
}

// val matchSummaryT = pivotSummary(matchSummary)
// val missSummaryT = pivotSummary(missSummary)
```

```scala
matchSummaryT.createOrReplaceTempView("match_desc")
missSummaryT.createOrReplaceTempView("miss_desc")
spark.sql("""
	SELECT a.field, a.count + b.count total, a.mean - b.mean delta
  FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field
	WHERE a.field NOT IN ("id_1", "id_2")
	ORDER BY delta DESC, total DESC
""").show()

case class MatchData(
	id_1: Int,
	id_2: Int,
	cmp_fname_c1: Option[Double],
	cmp_fname_c2: Option[Double],
	cmp_lname_c1: Option[Double],
	cmp_lname_c2: Option[Double],
	cmp_sex: Option[Int],
	cmp_bd: Option[Int],
	cmp_bm: Option[Int],
	cmp_by: Option[Int],
	cmp_plz: Option[Int],
	is_match: Boolean
)

val matchData = parsed.as[MatchData]
```

 The major difference between the two is that when we call functions like map, flatMap, or filter against matchData, we are processing instances of the MatchData case class instead of the Row class.

For our scoring function, we are going to sum up the value of one field of type Option[Double] (cmp_lname_c1) and four fields of type Option[Int] (cmp_plz, cmp_by, cmp_bd, and cmp_bm). 

```scala
case class Score(value: Double) = {
  def +(oi: Option[Int]) = {
    Score(value + oi.getOrElse(0))
  }
}

def scoreMatchData(md: MatchData): Double = {
	(Score(md.cmp_lname_c1.getOrElse(0.0)) + md.cmp_plz +
	md.cmp_by + md.cmp_bd + md.cmp_bm).value
}

val scored = matchData.map { md =>
	(scoreMatchData(md), md.is_match)
}.toDF("score", "is_match")

def crossTabs(scored: DataFrame, t: Double): DataFrame = {
  scored.
    selectExpr(s"score >= $t as above", "is_match").
    groupBy("above").
    pivot("is_match", Seq("true", "false")).
    count()
}
```



![](https://raw.githubusercontent.com/massquantity/Spark-advanced/master/pic/2.png?token=AG3LPEEJEDNWAYKKY6654Z25EW7L2)

















